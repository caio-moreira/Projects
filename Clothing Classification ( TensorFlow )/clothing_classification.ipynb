{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow to predict clothing type\n",
    "\n",
    "This notebook is a test of the use of Convolutional Neural Networks to predict clothing types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The data we'll use is a sample dataset included with TensorFlow called Fashion MNIST, which is a clothing type images dataset, it has 70000 28x28 images, 60000 for training and 10000 for testing, containing some types of clothes like shirts or shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_imgs_raw, train_labels), (test_imgs_raw, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We need to binarize data so that it will be easily worked on by the model.\n",
    "\n",
    "Since we are going to binarize the data using, which expects 2-dimensional data, we need to do that to each 28x28 matrix of the initially 3-dimensional data.\n",
    "\n",
    "In order to do that, we need to have a copy of the original data, because it is not writeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs_raw.copy()\n",
    "test_imgs = test_imgs_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a 75 threshold to binarize, which means every number higher than 75 (our data ranges from 0 to 255) will beceome 1, else 0.\n",
    "\n",
    "Why 75? after some tests it was the number that showed the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(75)\n",
    "\n",
    "for index, matrix in enumerate(train_imgs_raw):\n",
    "    train_imgs[index] = binarizer.fit_transform(matrix)\n",
    "\n",
    "for index, matrix in enumerate(test_imgs_raw):\n",
    "    test_imgs[index] = binarizer.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to show an exemple of data before and after binarizing so that we can see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2579edd7160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2579ee6bef0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be using convolutions, we have to reshape data to 4 dimensions so that we don't get any errors because the shape won't be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs.reshape(60000, 28, 28, 1)\n",
    "test_imgs = test_imgs.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Callback\n",
    "\n",
    "I'll define a Callback that stops the training after the model has reached 91% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\" This is a class that defines a Custom Callback.\n",
    "    \n",
    "    It will be used to stop training when it has reached a certain level of accuracy\n",
    "    \n",
    "    References can be found on https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/callbacks/Callback\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        ''' This function sets a task to happen whenever en epoch has finished during training.\n",
    "        \n",
    "        It will be used to check if accuracy has reached a certain level, if so, stops the model training.\n",
    "        \n",
    "        References can be found on https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/callbacks/Callback#on_epoch_end\n",
    "        \n",
    "        Keyword arguments:\n",
    "            epoch -- integer, index of epoch.\n",
    "            logs -- dict, metric results for this training epoch,\n",
    "                and for the validation epoch if validation is performed.\n",
    "                Validation result keys are prefixed with val_.\n",
    "        '''\n",
    "        \n",
    "        if(logs.get('accuracy') > 0.91):\n",
    "          print(f'\\n91% accuracy reached. Stopping training after {epoch + 1} epochs.')\n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Evaluating\n",
    "\n",
    "Calling the fit function for 5 epochs and passing a CustomCallback instance.\n",
    "\n",
    "After that, calling the evaluate function and printing evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 59s 991us/sample - loss: 0.4397 - accuracy: 0.8401\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 52s 875us/sample - loss: 0.3095 - accuracy: 0.8852\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 49s 822us/sample - loss: 0.2544 - accuracy: 0.9049\n",
      "Epoch 4/5\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9194\n",
      "91% accuracy reached. Stopping training after 4 epochs.\n",
      "60000/60000 [==============================] - 48s 805us/sample - loss: 0.2119 - accuracy: 0.9194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2579f0c1320>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_imgs, train_labels, epochs=5, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3381506258189678 - accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_imgs, test_labels, verbose=0)\n",
    "\n",
    "print('loss:', evaluation[0], '- accuracy:', evaluation[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
