{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow to predict clothing type\n",
    "\n",
    "This notebook is a test of the use of Convolutional Neural Networks to predict clothing types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.2.0\n",
      "Built with GPU Support? True\n",
      "Built with CUDA Support? True\n",
      "GPU Device Name: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Checking for GPU support and availability,\n",
    "# if device name isn't an empty string it runs on the GPU.\n",
    "print(f'''TensorFlow {tf.__version__}\n",
    "Built with GPU Support? {tf.test.is_built_with_gpu_support()}\n",
    "Built with CUDA Support? {tf.test.is_built_with_cuda()}\n",
    "GPU Device Name: {tf.test.gpu_device_name()}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "The data we'll use is a sample dataset included with TensorFlow called Fashion MNIST, which is a clothing type images dataset, it has 70000 28x28 images, 60000 for training and 10000 for testing, containing some types of clothes like shirts or shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_imgs_raw, train_labels), (test_imgs_raw, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We need to binarize data so that it will be easily worked on by the model.\n",
    "\n",
    "Since we are going to binarize the data using, which expects 2-dimensional data, we need to do that to each 28x28 matrix of the initially 3-dimensional data.\n",
    "\n",
    "In order to do that, we need to have a copy of the original data, because it is not writeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs_raw.copy()\n",
    "test_imgs = test_imgs_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using a 75 threshold to binarize, which means every number higher than 75 (our data ranges from 0 to 255) will beceome 1, else 0.\n",
    "\n",
    "Why 75? after some tests it was the number that showed the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=75)\n",
    "\n",
    "for index, matrix in enumerate(train_imgs_raw):\n",
    "    train_imgs[index] = binarizer.fit_transform(matrix)\n",
    "\n",
    "for index, matrix in enumerate(test_imgs_raw):\n",
    "    test_imgs[index] = binarizer.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to show an exemple of data before and after binarizing so that we can see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs_raw[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAK40lEQVR4nO3dT4ic933H8fenrqyAk4JU10Z2TJMGH2oKVcriFlyKi2ns+CLnkBIdggsG5RBDAjnUpIf4aEqT0EMJKLWIWlKHQGKsg2kiRMDkYrw2qi1Xbe0aNVEkpAYf4hQqy863h31U1vL+88wzf6zv+wXLzDwzu/Pdkd56ZueZ1S9VhaRr368tegBJ82HsUhPGLjVh7FITxi418evzvLPrs7s+wA3zvEuplf/lf3izLmWj66aKPcl9wN8C1wF/X1WPbXX7D3ADf5h7prlLSVt4tk5set3ET+OTXAf8HfBJ4A7gYJI7Jv16kmZrmp/Z7wRerarXqupN4DvAgXHGkjS2aWK/Ffjpustnh23vkORQktUkq5e5NMXdSZrGNLFv9CLAu957W1WHq2qlqlZ2sXuKu5M0jWliPwvctu7yh4Fz040jaVamif054PYkH01yPfAZ4Ng4Y0ka28SH3qrqrSQPAz9g7dDbkap6ebTJJI1qquPsVfU08PRIs0iaId8uKzVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhNTLdmc5AzwBvA28FZVrYwxlKTxTRX74E+r6ucjfB1JM+TTeKmJaWMv4IdJnk9yaKMbJDmUZDXJ6mUuTXl3kiY17dP4u6rqXJKbgONJ/q2qnll/g6o6DBwG+I3srSnvT9KEptqzV9W54fQi8CRw5xhDSRrfxLEnuSHJh66cBz4BnBprMEnjmuZp/M3Ak0mufJ1/qqp/HmUqSaObOPaqeg34/RFnkTRDHnqTmjB2qQljl5owdqkJY5eaMHapCWOXmjB2qQljl5owdqkJY5eaMHapCWOXmhjjP5yUJvKDcye3vP7eW/bPaZL52+p7n9X37Z5dasLYpSaMXWrC2KUmjF1qwtilJoxdasLj7Jqp7Y6lT/O5y3wcfprve1bcs0tNGLvUhLFLTRi71ISxS00Yu9SEsUtNeJxd71vLeCx7mW27Z09yJMnFJKfWbdub5HiSV4bTPbMdU9K0dvI0/lvAfVdtewQ4UVW3AyeGy5KW2LaxV9UzwOtXbT4AHB3OHwUeGHkuSSOb9AW6m6vqPMBwetNmN0xyKMlqktXLXJrw7iRNa+avxlfV4apaqaqVXeye9d1J2sSksV9Isg9gOL043kiSZmHS2I8BDw7nHwSeGmccSbOy7XH2JE8AdwM3JjkLfAV4DPhukoeAnwCfnuWQWl4e637/2Db2qjq4yVX3jDyLpBny7bJSE8YuNWHsUhPGLjVh7FIT/oqrtuShtWuHe3apCWOXmjB2qQljl5owdqkJY5eaMHapCY+zN+dx9D7cs0tNGLvUhLFLTRi71ISxS00Yu9SEsUtNeJz9Gudx9Pef7f7M7r1l/0Rf1z271ISxS00Yu9SEsUtNGLvUhLFLTRi71ITH2a8BHkvXTmy7Z09yJMnFJKfWbXs0yc+SnBw+7p/tmJKmtZOn8d8C7ttg+9erav/w8fS4Y0ka27axV9UzwOtzmEXSDE3zAt3DSV4cnubv2exGSQ4lWU2yeplLU9ydpGlMGvs3gI8B+4HzwFc3u2FVHa6qlapa2cXuCe9O0rQmir2qLlTV21X1K+CbwJ3jjiVpbBPFnmTfuoufAk5tdltJy2Hb4+xJngDuBm5Mchb4CnB3kv1AAWeAz81wxmuex8k1D9vGXlUHN9j8+AxmkTRDvl1WasLYpSaMXWrC2KUmjF1qwl9xHXj4S9c69+xSE8YuNWHsUhPGLjVh7FITxi41YexSE++r4+weC5cm555dasLYpSaMXWrC2KUmjF1qwtilJoxdamKpjrN7HF2aHffsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjWxbexJbkvyoySnk7yc5AvD9r1Jjid5ZTjdM/txJU1qJ3v2t4AvVdXvAn8EfD7JHcAjwImquh04MVyWtKS2jb2qzlfVC8P5N4DTwK3AAeDocLOjwAOzGlLS9N7Tz+xJPgJ8HHgWuLmqzsPaPwjATZt8zqEkq0lWL3NpumklTWzHsSf5IPA94ItV9Yudfl5VHa6qlapa2cXuSWaUNIIdxZ5kF2uhf7uqvj9svpBk33D9PuDibEaUNIZtf8U1SYDHgdNV9bV1Vx0DHgQeG06fmnaYe2/ZP+2XmJi/Xjsb/pkuj538PvtdwGeBl5JcefS+zFrk303yEPAT4NOzGVHSGLaNvap+DGSTq+8ZdxxJs+I76KQmjF1qwtilJoxdasLYpSaW6r+SXqRFHg/WbPhn+k7u2aUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSa2jT3JbUl+lOR0kpeTfGHY/miSnyU5OXzcP/txJU1qJ4tEvAV8qapeSPIh4Pkkx4frvl5VfzO78SSNZSfrs58Hzg/n30hyGrh11oNJGtd7+pk9yUeAjwPPDpseTvJikiNJ9mzyOYeSrCZZvcylqYaVNLkdx57kg8D3gC9W1S+AbwAfA/aztuf/6kafV1WHq2qlqlZ2sXuEkSVNYkexJ9nFWujfrqrvA1TVhap6u6p+BXwTuHN2Y0qa1k5ejQ/wOHC6qr62bvu+dTf7FHBq/PEkjWUnr8bfBXwWeCnJyWHbl4GDSfYDBZwBPjeTCSWNYievxv8YyAZXPT3+OJJmxXfQSU0Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9REqmp+d5b8N/Bf6zbdCPx8bgO8N8s627LOBc42qTFn++2q+q2Nrphr7O+682S1qlYWNsAWlnW2ZZ0LnG1S85rNp/FSE8YuNbHo2A8v+P63sqyzLetc4GyTmstsC/2ZXdL8LHrPLmlOjF1qYiGxJ7kvyb8neTXJI4uYYTNJziR5aViGenXBsxxJcjHJqXXb9iY5nuSV4XTDNfYWNNtSLOO9xTLjC33sFr38+dx/Zk9yHfAfwJ8BZ4HngINV9a9zHWQTSc4AK1W18DdgJPkT4JfAP1TV7w3b/hp4vaoeG/6h3FNVf7kksz0K/HLRy3gPqxXtW7/MOPAA8Bcs8LHbYq4/Zw6P2yL27HcCr1bVa1X1JvAd4MAC5lh6VfUM8PpVmw8AR4fzR1n7yzJ3m8y2FKrqfFW9MJx/A7iyzPhCH7st5pqLRcR+K/DTdZfPslzrvRfwwyTPJzm06GE2cHNVnYe1vzzATQue52rbLuM9T1ctM740j90ky59PaxGxb7SU1DId/7urqv4A+CTw+eHpqnZmR8t4z8sGy4wvhUmXP5/WImI/C9y27vKHgXMLmGNDVXVuOL0IPMnyLUV94coKusPpxQXP8/+WaRnvjZYZZwkeu0Uuf76I2J8Dbk/y0STXA58Bji1gjndJcsPwwglJbgA+wfItRX0MeHA4/yDw1AJneYdlWcZ7s2XGWfBjt/Dlz6tq7h/A/ay9Iv+fwF8tYoZN5vod4F+Gj5cXPRvwBGtP6y6z9ozoIeA3gRPAK8Pp3iWa7R+Bl4AXWQtr34Jm+2PWfjR8ETg5fNy/6Mdui7nm8rj5dlmpCd9BJzVh7FITxi41YexSE8YuNWHsUhPGLjXxf2mEbTIqMQw7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_imgs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to be using convolutions, we have to reshape data to 4 dimensions so that we don't get any errors because the shape won't be recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs.reshape(60000, 28, 28, 1)\n",
    "test_imgs = test_imgs.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Callback\n",
    "\n",
    "I'll define a Callback that stops the training after the model has reached 91% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\" This is a class that defines a Custom Callback.\n",
    "    \n",
    "    It will be used to stop training when it has reached a certain level of accuracy\n",
    "    \n",
    "    References can be found on https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/callbacks/Callback\n",
    "    \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        ''' This function sets a task to happen whenever en epoch has finished during training.\n",
    "        \n",
    "        It will be used to check if accuracy has reached a certain level, if so, stops the model training.\n",
    "        \n",
    "        References can be found on https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/callbacks/Callback#on_epoch_end\n",
    "        '''\n",
    "        \n",
    "        if(logs.get('acc') > 0.90):\n",
    "          print(f'\\n90% accuracy reached. Stopping training after {epoch + 1} epochs.')\n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Evaluating\n",
    "\n",
    "Calling the fit function for 5 epochs and passing a CustomCallback instance.\n",
    "\n",
    "After that, calling the evaluate function and printing evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.4523 - acc: 0.8361\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.3250 - acc: 0.8809\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.2778 - acc: 0.8966\n",
      "Epoch 4/5\n",
      "933/938 [============================>.] - ETA: 0s - loss: 0.2376 - acc: 0.9107\n",
      "90% accuracy reached. Stopping training after 4 epochs.\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.2375 - acc: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2e1ca9ece80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_imgs, train_labels, epochs=5, batch_size=64, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.34990066289901733 - accuracy: 0.8773000240325928\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_imgs, test_labels, verbose=0)\n",
    "\n",
    "print('loss:', evaluation[0], '- accuracy:', evaluation[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
